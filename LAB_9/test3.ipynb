{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset and preprocess\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "\n",
    "# One-hot encode labels\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# Reshape images to (28, 28, 1) for Conv2D layer\n",
    "train_images_reshaped = train_images.reshape((-1, 28, 28, 1))\n",
    "test_images_reshaped = test_images.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range = 12, width_shift_range = 0.1, shear_range = 0.1,\n",
    "                         height_shift_range = 0.1, zoom_range = 0.1, fill_mode = 'nearest', horizontal_flip = False,\n",
    "                         vertical_flip = False, featurewise_center = False,\n",
    "                         samplewise_center = False, featurewise_std_normalization = False,\n",
    "                         samplewise_std_normalization = False)\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "# Create batches to  train models faster\n",
    "train_generator = gen.flow(train_images_reshaped,train_labels, batch_size = 32)\n",
    "test_generator = test_gen.flow(test_images_reshaped, test_labels, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', patience = 3, verbose = 1, factor = 0.4, min_lr = 0.00002,\n",
    "                                            mode = 'auto', cooldown = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashuo\\AppData\\Local\\Temp\\ipykernel_33420\\1689458331.py:46: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator, steps_per_epoch = 40000//16, epochs = 10,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/2500 [=====================>........] - ETA: 47s - loss: 0.4856 - accuracy: 0.8877WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset.\n",
      "2500/2500 [==============================] - 150s 60ms/step - loss: 0.4856 - accuracy: 0.8877 - val_loss: 4.1094 - val_accuracy: 0.6338 - lr: 0.0100\n",
      "313/313 - 5s - loss: 4.1094 - accuracy: 0.6338 - 5s/epoch - 17ms/step\n",
      "Model-1 test accuracy: 0.6338\n",
      "Epoch 1/10\n",
      "1875/2500 [=====================>........] - ETA: 51s - loss: 0.4783 - accuracy: 0.8911WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset.\n",
      "2500/2500 [==============================] - 162s 64ms/step - loss: 0.4783 - accuracy: 0.8911 - val_loss: 0.1084 - val_accuracy: 0.9683 - lr: 0.0100\n",
      "313/313 - 6s - loss: 0.1084 - accuracy: 0.9683 - 6s/epoch - 20ms/step\n",
      "Model-2 test accuracy: 0.9683\n",
      "Epoch 1/10\n",
      "1875/2500 [=====================>........] - ETA: 47s - loss: 0.4490 - accuracy: 0.8995WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset.\n",
      "2500/2500 [==============================] - 151s 60ms/step - loss: 0.4490 - accuracy: 0.8995 - val_loss: 0.0590 - val_accuracy: 0.9834 - lr: 0.0100\n",
      "313/313 - 5s - loss: 0.0590 - accuracy: 0.9834 - 5s/epoch - 18ms/step\n",
      "Model-3 test accuracy: 0.9834\n",
      "Epoch 1/10\n",
      "1875/2500 [=====================>........] - ETA: 47s - loss: 0.4471 - accuracy: 0.9014WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset.\n",
      "2500/2500 [==============================] - 150s 60ms/step - loss: 0.4471 - accuracy: 0.9014 - val_loss: 0.0560 - val_accuracy: 0.9825 - lr: 0.0100\n",
      "313/313 - 5s - loss: 0.0560 - accuracy: 0.9825 - 5s/epoch - 17ms/step\n",
      "Model-4 test accuracy: 0.9825\n",
      "Epoch 1/10\n",
      "1875/2500 [=====================>........] - ETA: 47s - loss: 0.4654 - accuracy: 0.8960WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1250 batches). You may need to use the repeat() function when building your dataset.\n",
      "2500/2500 [==============================] - 150s 60ms/step - loss: 0.4654 - accuracy: 0.8960 - val_loss: 0.0707 - val_accuracy: 0.9795 - lr: 0.0100\n",
      "313/313 - 5s - loss: 0.0707 - accuracy: 0.9795 - 5s/epoch - 17ms/step\n",
      "Model-5 test accuracy: 0.9795\n",
      "Mean Test Accuracy across Models: 0.909500002861023\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D\n",
    "from keras import models\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
    "\n",
    "# Function to create and train the specified CNN model\n",
    "def create_and_train_model():\n",
    "    \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last',\n",
    "                        input_shape=(28,28,1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid' ))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu', data_format='channels_last'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='valid', strides=2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.01), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#     # Reshape images to (28, 28, 1) for Conv2D layer\n",
    "#      train_images_reshaped = train_images.reshape((-1, 28, 28, 1))\n",
    "#      test_images_reshaped = test_images.reshape((-1, 28, 28, 1))\n",
    "\n",
    "        history = model.fit_generator(train_generator, steps_per_epoch = 40000//16, epochs = 10, \n",
    "                              validation_data = test_generator, validation_steps = 10000//8, verbose = 1,\n",
    "                              callbacks=[reduce_lr])\n",
    "\n",
    "        # evaluate the model\n",
    "        scores = model.evaluate(test_images_reshaped, test_labels, verbose = 2)\n",
    "\n",
    "        return scores[1]\n",
    "\n",
    "num_models = 5\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    acurracy = create_and_train_model()\n",
    "    test_accuracies.append(acurracy)\n",
    "    print(f'Model-{i+1} test accuracy: {acurracy:.4f}')\n",
    "\n",
    "# Compute the mean test accuracy\n",
    "mean_test_accuracy = np.mean(test_accuracies)\n",
    "print(f'Mean Test Accuracy across Models: {mean_test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebook",
   "language": "python",
   "name": "jupyternotebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
