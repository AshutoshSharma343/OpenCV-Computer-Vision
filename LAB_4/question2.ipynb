{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner import hp\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to between 0 and 1\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Function to create a model for hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "\n",
    "     # Add the first hidden layer with neurons varying between 250 to 260 with a step value of 2\n",
    "    hp_neurons = hp.Int('neurons', min_value=250, max_value=260, step=2)\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    model.add(layers.Dense(units=hp_neurons, activation='sigmoid'))\n",
    "\n",
    "    # Add the second hidden layer with 16 neurons and activation function tanh\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "\n",
    "    # Add the third hidden layer with 8 neurons and activation function relu\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "\n",
    "    # Add the fourth hidden layer with 4 neurons and activation function selu\n",
    "    model.add(Dense(4, activation='selu'))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.1, 0.01, 0.15])\n",
    "    # Compile model\n",
    "    model.compile(optimizer=SGD(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the KerasTuner RandomSearch tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of hyperparameter combinations to try\n",
    "    directory='tuner_logs',\n",
    "    project_name='mnist_tuning'\n",
    ")\n",
    "\n",
    "hp_batch_size = hp.Choice('batch_size', values=[4, 8, 16])\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_test, y_test), batch_size=hp_batch_size)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hps)\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "best_model = tuner.oracle.get_best_trials(num_trials=1)[0].hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(x_train, y_train, epochs=10, batch_size=best_hps['batch_size'], validation_split=0.1)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get the best hyperparameters\n",
    "# best_hps = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "# print(\"Best Hyperparameters:\")\n",
    "# print(best_hps)\n",
    "\n",
    "# # Build the model with the best hyperparameters\n",
    "# best_model = tuner.oracle.get_best_trials(num_trials=1)[0].hypermodel.build(best_hps)\n",
    "\n",
    "# # Train the best model on the entire training set\n",
    "# best_model.fit(x_train, y_train, epochs=10, batch_size=best_hps['batch_size'], validation_split=0.1)\n",
    "\n",
    "# # Evaluate the best model on the test set\n",
    "# test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "# print(f\"\\nTest Accuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebook",
   "language": "python",
   "name": "jupyternotebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
